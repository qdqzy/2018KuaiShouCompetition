{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改一下原数据集的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf-8')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: by argument to sort_index is deprecated, please use .sort_values(by=...)\n",
      "  \"\"\"\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: by argument to sort_index is deprecated, please use .sort_values(by=...)\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:15: FutureWarning: by argument to sort_index is deprecated, please use .sort_values(by=...)\n",
      "  from ipykernel import kernelapp as app\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: by argument to sort_index is deprecated, please use .sort_values(by=...)\n"
     ]
    }
   ],
   "source": [
    "#now_data = 'aline'\n",
    "#'data/' + now_data +\n",
    "\n",
    "app_launch_log = pd.read_csv( 'app_launch_log.txt',sep='\\t',header=None)\n",
    "app_launch_log = app_launch_log.sort_index(by=[0,1])\n",
    "app_launch_log = app_launch_log.rename({0:'user_id',1:'launch_day'},axis=1)\n",
    "app_launch_log.to_csv('app_launch_log.csv',index=False)\n",
    "\n",
    "user_activity_log = pd.read_csv('user_activity_log.txt',sep='\\t',header=None)\n",
    "user_activity_log = user_activity_log.sort_index(by=[0,1])\n",
    "user_activity_log = user_activity_log.rename({0:'user_id',1:'activity_day',2:'page',3:'video_id',4:'author_id',5:'action_type'},axis=1)\n",
    "user_activity_log.to_csv('user_activity_log.csv',index=False)\n",
    "\n",
    "user_register_log = pd.read_csv( 'user_register_log.txt',sep='\\t',header=None)\n",
    "user_register_log = user_register_log.sort_index(by=[0,1])\n",
    "user_register_log = user_register_log.rename({0:'user_id',1:'register_day',2:'register_type',3:'device type'},axis=1)\n",
    "user_register_log.to_csv('user_register_log.csv',index=False)\n",
    "\n",
    "video_create_log = pd.read_csv( 'video_create_log.txt',sep='\\t',header=None)\n",
    "video_create_log = video_create_log.sort_index(by=[0,1])\n",
    "video_create_log = video_create_log.rename({0:'user_id',1:'create_day'},axis=1)\n",
    "video_create_log.to_csv('video_create_log.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分数据集部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始划分数据集......\n",
      "根据起始和结束时间将数据集取出完成，存入： one_dataSet_train_\n",
      "根据起始和结束时间将数据集取出完成，存入： one_dataSet_test_\n",
      "第一数据集（包括数据集和结果集）划分完成......\n",
      "根据起始和结束时间将数据集取出完成，存入： two_dataSet_train_\n",
      "根据起始和结束时间将数据集取出完成，存入： two_dataSet_test_\n",
      "第二数据集（包括数据集和结果集）划分完成......\n",
      "根据起始和结束时间将数据集取出完成，存入： three_dataSet_train_\n",
      "第三数据集（包括数据集和结果集）划分完成......\n",
      "成功划分三个数据集......\n"
     ]
    }
   ],
   "source": [
    "all_dataSet_path = 'all_dataSet.csv'\n",
    "one_dataSet_train_path = 'one_dataSet_train_'\n",
    "one_dataSet_test_path = 'one_dataSet_test_'\n",
    "two_dataSet_train_path = 'two_dataSet_train_'\n",
    "two_dataSet_test_path = 'two_dataSet_test_'\n",
    "three_dataSet_train_path = 'three_dataSet_train_'\n",
    "\n",
    "launch = pd.read_csv('app_launch_log.csv')\n",
    "activity = pd.read_csv('user_activity_log.csv')\n",
    "register = pd.read_csv('user_register_log.csv')\n",
    "video = pd.read_csv('video_create_log.csv')\n",
    "\n",
    "def cut_data_as_time(dataSet_path, new_dataSet_path, begin_day, end_day):\n",
    "    temp_register = register[(register['register_day'] >= begin_day) & (register['register_day'] <= end_day)]\n",
    "    temp_create = video[(video['create_day'] >= begin_day) & (video['create_day'] <= end_day)]\n",
    "    temp_launch = launch[(launch['launch_day'] >= begin_day) & (launch['launch_day'] <= end_day)]\n",
    "    temp_activity = activity[(activity['activity_day'] >= begin_day) & (activity['activity_day'] <= end_day)]\n",
    "    \n",
    "    temp_register.to_csv(new_dataSet_path + 'register.csv',index=False)\n",
    "    temp_create.to_csv(new_dataSet_path + 'create.csv',index=False)\n",
    "    temp_launch.to_csv(new_dataSet_path + 'launch.csv',index=False)\n",
    "    temp_activity.to_csv(new_dataSet_path + 'activity.csv',index=False)\n",
    "\n",
    "    print(u'根据起始和结束时间将数据集取出完成，存入：',new_dataSet_path)\n",
    "\n",
    "def generate_dataSet():\n",
    "    print(\"开始划分数据集......\")\n",
    "    # 以1-16数据 预测17-23某用户是否活跃\n",
    "    begin_day = 1\n",
    "    end_day = 16\n",
    "    cut_data_as_time(all_dataSet_path,one_dataSet_train_path,begin_day,end_day)\n",
    "    begin_day = 17\n",
    "    end_day = 23\n",
    "    cut_data_as_time(all_dataSet_path,one_dataSet_test_path,begin_day,end_day)\n",
    "    print(\"第一数据集（包括数据集和结果集）划分完成......\")\n",
    "    # 以8-23数据 预测24-30某用户是否活跃\n",
    "    begin_day = 8\n",
    "    end_day = 23\n",
    "    cut_data_as_time(all_dataSet_path,two_dataSet_train_path,begin_day,end_day)\n",
    "    begin_day = 24\n",
    "    end_day = 30\n",
    "    cut_data_as_time(all_dataSet_path,two_dataSet_test_path,begin_day,end_day)\n",
    "    print(\"第二数据集（包括数据集和结果集）划分完成......\")\n",
    "    # 以15-30数据 预测31-37某用户是否活跃\n",
    "    begin_day = 15\n",
    "    end_day = 30\n",
    "    cut_data_as_time(all_dataSet_path,three_dataSet_train_path,begin_day,end_day)\n",
    "    print(\"第三数据集（包括数据集和结果集）划分完成......\")\n",
    "    print(\"成功划分三个数据集......\")\n",
    "    \n",
    "generate_dataSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.stats import stats\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_dataSet_train_path = 'one_dataSet_train_'\n",
    "one_dataSet_test_path = 'one_dataSet_test_'\n",
    "two_dataSet_train_path = 'two_dataSet_train_'\n",
    "two_dataSet_test_path = 'two_dataSet_test_'\n",
    "three_dataSet_train_path = 'three_dataSet_train_'\n",
    "\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "\n",
    "register = 'register.csv'\n",
    "create = 'create.csv'\n",
    "launch = 'launch.csv'\n",
    "activity = 'activity.csv'\n",
    "\n",
    "# 构建训练集与测试集与特征\n",
    "# 获取所有id 查看对应id是否在测试集里面出现过\n",
    "def get_train_label(train_path,test_path):\n",
    "    train_reg = pd.read_csv(train_path + register,usecols = ['user_id'])\n",
    "    train_cre = pd.read_csv(train_path + create,usecols = ['user_id'])\n",
    "    train_lau = pd.read_csv(train_path + launch,usecols = ['user_id'])\n",
    "    train_act = pd.read_csv(train_path + activity,usecols = ['user_id'])\n",
    "    train_data_id = np.unique(pd.concat([train_reg,train_cre,train_lau,train_act]))\n",
    "    \n",
    "    test_reg = pd.read_csv(test_path + register,usecols = ['user_id'])\n",
    "    test_cre = pd.read_csv(test_path + create,usecols = ['user_id'])\n",
    "    test_lau = pd.read_csv(test_path + launch,usecols = ['user_id'])\n",
    "    test_act = pd.read_csv(test_path + activity,usecols = ['user_id'])\n",
    "    test_data_id = np.unique(pd.concat([test_reg,test_cre,test_lau,test_act]))\n",
    "    \n",
    "    train_label = []\n",
    "    for i in train_data_id:\n",
    "        for i in test_data_id:\n",
    "            train_label.append(1)\n",
    "        else:\n",
    "            train_label.append(0)\n",
    "    train_data = pd.DataFrame()\n",
    "    train_data['user_id'] = train_data_id\n",
    "    train_data['label'] = train_label\n",
    "    return train_data\n",
    "\n",
    "def get_test(test_path):\n",
    "    test_reg = pd.read_csv(test_path + register,usecols = ['user_id'])\n",
    "    test_cre = pd.read_csv(test_path + create,usecols = ['user_id'])\n",
    "    test_lau = pd.read_csv(test_path + launch,usecols = ['user_id'])\n",
    "    test_act = pd.read_csv(test_path + activity,usecols = ['user_id'])\n",
    "    test_data_id = np.unique(pd.concat([test_reg,test_cre,test_lau,test_act]))\n",
    "    test_data = pd.DataFrame()\n",
    "    test_data['user_id'] = test_data_id\n",
    "    return test_data\n",
    "\n",
    "def get_create_feature(row):\n",
    "    feature = pd.Series()\n",
    "    feature['user_id'] = list(row['user_id'])[0]\n",
    "    return feature\n",
    "\n",
    "def get_register_feature(row):\n",
    "    feature = pd.Series()\n",
    "    feature['user_id'] = list(row['user_id'])[0]\n",
    "    return feature\n",
    "\n",
    "def get_launch_feature(row):\n",
    "    feature = pd.Series()\n",
    "    feature['user_id'] = list(row['user_id'])[0]\n",
    "    return feature\n",
    "\n",
    "def get_activity_feature(row):\n",
    "    feature = pd.Series()\n",
    "    feature['user_id'] = list(row['user_id'])[0]\n",
    "    return feature\n",
    "\n",
    "def deal_feature(path,user_id):\n",
    "    reg = pd.read_csv(path + register)\n",
    "    cre = pd.read_csv(path + create)\n",
    "    lau = pd.read_csv(path + launch)\n",
    "    act = pd.read_csv(path + activity)\n",
    "    feature = pd.DataFrame()\n",
    "    feature['user_id'] = user_id\n",
    "    \n",
    "    # w...\n",
    "    cre['max_day'] = np.max(reg['register_day'])\n",
    "    cre_feature = cre.groupby('user_id',sort=Ture).apply(get_create_feature)\n",
    "    feature = pd.merge(feature,pd.DataFrame(cre_feature),on='user_id',how='left')\n",
    "    print(\"create表特征提取完毕\")\n",
    "    \n",
    "    # w...\n",
    "    reg['max_day'] = np.max(reg['register_day'])\n",
    "    reg_feature = reg.groupby('user_id',sort=Ture).apply(get_register_feature)\n",
    "    feature = pd.merge(feature,pd.DataFrame(reg_feature),on='user_id',how='left')\n",
    "    print(\"register表特征提取完毕\")\n",
    "    \n",
    "    # w...\n",
    "    lau['max_day'] = np.max(reg['register_day'])\n",
    "    lau_feature = lau.groupby('user_id',sort=Ture).apply(get_launch_feature)\n",
    "    feature = pd.merge(feature,pd.DataFrame(lau_feature),on='user_id',how='left')\n",
    "    print(\"launch表特征提取完毕\")\n",
    "    \n",
    "    # w......\n",
    "    act['max_day'] = np.max(reg['register_day'])\n",
    "    act_feature = act.groupby('user_id',sort=Ture).apply(get_activity_feature)\n",
    "    feature = pd.merge(feature,pd.DataFrame(act_feature),on='user_id',how='left')\n",
    "    print(\"activity表特征提取完毕\")\n",
    "    return feature\n",
    "\n",
    "# 构建特征\n",
    "def get_data_feature():\n",
    "    one_train_data = get_train_label(one_dataSet_train_path,one_dataSet_test_path)\n",
    "    one_feature = deal_feature(one_dataSet_train_path,one_train_data['user_id'])\n",
    "    one_feature['label'] = one_train_data['label']\n",
    "    print(\"第一组训练数据特征提取完毕\")\n",
    "    \n",
    "    two_train_data = get_train_label(two_dataSet_train_path,two_dataSet_test_path)\n",
    "    two_feature = deal_feature(two_dataSet_train_path,two_train_data['user_id'])\n",
    "    two_feature['label'] = two_train_data['label']\n",
    "    print(\"第二组训练数据特征提取完毕\")\n",
    "    \n",
    "    train_feature = pd.concat([one_feature,two_feature])\n",
    "    train_feature.to_csv(train_path,index=False)\n",
    "    print(\"训练数据存储完毕\")\n",
    "    \n",
    "    test_data = get_test(three_dataSet_train_path)\n",
    "    test_feature = deal_feature(three_dataSet_train_path,test_data['user_id'])\n",
    "    test_feature.to_csv(test_path,index=False)\n",
    "    print(\"测试数据存储完毕\")\n",
    "\n",
    "get_data_feature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型预测（lgb）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "print('开始处理特征......')\n",
    "\n",
    "train_path = 'train_and_test/train.csv'\n",
    "test_path = 'train_and_test/test.csv'\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "used_feature = ['create_count', 'create_day_diff_mean', 'create_day_diff_std', 'create_day_diff_max',\n",
    "                'create_day_diff_min', 'create_mode', 'last_day_cut_max_day',\n",
    "                'register_type', 'device_type', 'register_day_cut_max_day',\n",
    "                'launch_count', 'launch_day_diff_mean', 'launch_day_diff_std',\n",
    "                'launch_day_diff_max', 'launch_day_diff_min', 'launch_day_diff_kur',\n",
    "                'launch_day_diff_ske', 'launch_day_diff_last', 'launch_day_cut_max_day',\n",
    "                'activity_count',\n",
    "                'activity_day_diff_mean',\n",
    "                'activity_day_diff_std',\n",
    "                'activity_day_diff_max', 'activity_day_diff_min', 'activity_day_diff_kur',\n",
    "                'activity_day_diff_ske',\n",
    "                'activity_day_diff_last',\n",
    "                '0_page_count', '1_page_count', '2_page_count', '3_page_count', '4_page_count',\n",
    "                '0_page_count_div_sum', '1_page_count_div_sum', '2_page_count_div_sum',\n",
    "                '3_page_count_div_sum', '4_page_count_div_sum',\n",
    "                '0_action_count',\n",
    "                '1_action_count', '2_action_count', '3_action_count', '4_action_count',\n",
    "                '5_action_count', '0_action_count_div_sum', '1_action_count_div_sum',\n",
    "                '2_action_count_div_sum', '3_action_count_div_sum',\n",
    "                '4_action_count_div_sum', '5_action_count_div_sum',\n",
    "                'video_id_mode', 'author_id_mode', 'activity_count_mean',\n",
    "                'activity_count_std', 'activity_count_max', 'activity_count_min',\n",
    "                'activity_count_kur', 'activity_count_ske', 'activity_count_last',\n",
    "                'activity_diff_count_mean', 'activity_diff_count_std', 'activity_diff_count_max',\n",
    "                'activity_diff_count_min', 'activity_diff_count_kur', 'activity_diff_count_ske',\n",
    "                'activity_diff_count_last', 'activity_page0_mean', 'activity_page0_std',\n",
    "                'activity_page0_max', 'activity_page0_min', 'activity_page0_kur', 'activity_page0_ske',\n",
    "                'activity_page0_last', 'activity_page1_mean', 'activity_page1_std', 'activity_page1_max',\n",
    "                'activity_page1_min', 'activity_page1_kur', 'activity_page1_ske', 'activity_page1_last',\n",
    "                'activity_page2_mean', 'activity_page2_std', 'activity_page2_max', 'activity_page2_min',\n",
    "                'activity_page2_kur', 'activity_page2_ske', 'activity_page2_last', 'activity_page3_mean',\n",
    "                'activity_page3_std', 'activity_page3_max', 'activity_page3_min', 'activity_page3_kur',\n",
    "                'activity_page3_ske', 'activity_page3_last', 'activity_page4_mean', 'activity_page4_std',\n",
    "                'activity_page4_max', 'activity_page4_min', 'activity_page4_kur', 'activity_page4_ske',\n",
    "                'activity_page4_last', 'activity_type0_mean', 'activity_type0_std', 'activity_type0_max',\n",
    "                'activity_type0_min', 'activity_type0_kur', 'activity_type0_ske', 'activity_type0_last',\n",
    "                'activity_type1_mean', 'activity_type1_std', 'activity_type1_max', 'activity_type1_min',\n",
    "                'activity_type1_kur', 'activity_type1_ske', 'activity_type1_last', 'activity_type2_mean',\n",
    "                'activity_type2_std', 'activity_type2_max', 'activity_type2_min', 'activity_type2_kur',\n",
    "                'activity_type2_ske', 'activity_type2_last', 'activity_type3_mean', 'activity_type3_std',\n",
    "                'activity_type3_max', 'activity_type3_min', 'activity_type3_kur', 'activity_type3_ske',\n",
    "                'activity_type3_last', 'activity_type4_mean', 'activity_type4_std', 'activity_type4_max',\n",
    "                'activity_type4_min', 'activity_type4_kur', 'activity_type4_ske', 'activity_type4_last',\n",
    "                'activity_type5_mean', 'activity_type5_std', 'activity_type5_max', 'activity_type5_min',\n",
    "                'activity_type5_kur', 'activity_type5_ske', 'activity_type5_last', 'activity_day_cut_max_day',\n",
    "                'max_activity_day',\n",
    "                 'create_sub_register', 'activity_sub_register', 'launch_sub_register',\n",
    "                ]\n",
    "used_feature = np.array(used_feature)\n",
    "print(used_feature)\n",
    "importance_feature = [21,60,54,71,106,44,50,27,33,58,43,11,19,35,64,32,45,9,37,143,142,10,7,18,8]\n",
    "used_feature = used_feature[np.array(importance_feature)]\n",
    "print(used_feature)\n",
    "train_feature = train[used_feature]\n",
    "test_feature = test[used_feature]\n",
    "label = train['label']\n",
    "\n",
    "# 切分训练\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(train_feature, label, test_size=0.2,random_state=1017)\n",
    "# train_feature = X_train\n",
    "# label = Y_train\n",
    "\n",
    "print('特征处理完毕......')\n",
    "\n",
    "\n",
    "###################### lgb ##########################\n",
    "import lightgbm as lgb\n",
    "\n",
    "print('载入数据......')\n",
    "lgb_train = lgb.Dataset(train_feature, label)\n",
    "lgb_eval = lgb.Dataset(X_test, Y_test, reference=lgb_train)\n",
    "\n",
    "\n",
    "print('开始训练......')\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'auc', 'binary_logloss'}\n",
    "}\n",
    "# params = {\n",
    "#     'task': 'train',\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'objective': 'regression',\n",
    "#     'metric': {'l2'}\n",
    "# }\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=58,\n",
    "                valid_sets=lgb_eval\n",
    "                )\n",
    "gbm.save_model('model/lgb_model.txt')\n",
    "\n",
    "temp = gbm.predict(X_test)\n",
    "temp[temp>0.5]=1\n",
    "temp[temp<0.5]=0\n",
    "print('结果：' + str(sklearn.metrics.f1_score(Y_test, temp)))\n",
    "print('特征重要性：'+ str(list(gbm.feature_importance())))\n",
    "\n",
    "\n",
    "########################## 保存结果 ############################\n",
    "pre = gbm.predict(test_feature)\n",
    "df_result = pd.DataFrame()\n",
    "df_result['user_id'] = test['user_id']\n",
    "df_result['result'] = pre\n",
    "df_result.to_csv('result/lgb_result.csv', index=False)\n",
    "pre[pre >= 0.5]=1\n",
    "pre[pre < 0.5]=0\n",
    "pre = map(int,pre)\n",
    "print('为1的个数：' + str(len(np.where(np.array(pre)==1)[0])))\n",
    "print('为0的个数：' + str(len(np.where(np.array(pre)==0)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "lgb = pd.read_csv('result/lgb_result.csv')\n",
    "print(lgb)\n",
    "res = lgb[lgb['result'] >= 0.5]\n",
    "print(len(res))\n",
    "del res['result']\n",
    "res.to_csv('result/dealed_result.txt', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
